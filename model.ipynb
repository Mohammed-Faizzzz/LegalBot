{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fitz\n",
    "!pip install pymupdf\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_data(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Extract Title\n",
    "    title_pattern = re.compile(r\"\\[\\d{4}\\] SGHC \\d+\")\n",
    "    title_match = title_pattern.search(text)\n",
    "    title = title_match.group(0) if title_match else \"Title not found\"\n",
    "\n",
    "    data = {\n",
    "        \"Title\": title,\n",
    "        \"Text\": text\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def process_multiple_pdfs(pdf_folder):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            data = extract_pdf_data(pdf_path)\n",
    "            all_data.append(data)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Save the extracted data to a CSV file\n",
    "    df.to_csv('extracted_data.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"pdfs\"\n",
    "df = process_multiple_pdfs(pdf_folder)\n",
    "# print(df)\n",
    "\n",
    "# Load the dataset\n",
    "titles = df['Title'].tolist()\n",
    "texts = df['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, titles, texts, tokenizer, max_length, overlap_length):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.overlap_length = overlap_length\n",
    "\n",
    "        # Preprocess each text to split into overlapping chunks and associate with titles\n",
    "        for title, text in zip(titles, texts):\n",
    "            tokens = tokenizer(text, truncation=True, return_tensors='pt')['input_ids'].squeeze()\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                end = min(start + max_length, len(tokens))\n",
    "                chunk = tokens[start:end]\n",
    "                if len(chunk) > 0:  # Ensure there is content in the chunk\n",
    "                    self.examples.append((title, chunk))\n",
    "                start += max_length - overlap_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title, chunk = self.examples[idx]\n",
    "\n",
    "        # Padding if necessary\n",
    "        padding_length = self.max_length - chunk.size(0)\n",
    "        if padding_length > 0:\n",
    "            chunk = torch.cat([chunk, torch.zeros(padding_length, dtype=torch.long)])\n",
    "\n",
    "        attention_mask = torch.ones(chunk.size(0), dtype=torch.long)\n",
    "        attention_mask[chunk == 0] = 0  # Set attention mask to 0 where there's padding\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'input_ids': chunk,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': chunk\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "max_length = 512\n",
    "overlap_length = 50  # Define the overlap length\n",
    "dataset = TextDataset(titles, texts, tokenizer, max_length, overlap_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the device is set correctly\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Clear CUDA memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = 'gpt2-medium'  # or 'gpt2' for the smaller model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with mixed precision\n",
    "num_epochs = 3\n",
    "accumulation_steps = 8\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clear up memory after each batch\n",
    "        del input_ids, attention_mask, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {epoch_loss / len(dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
